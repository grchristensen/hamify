{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import tarfile\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "import re\n",
    "import pandas as pd\n",
    "from os import path\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the first 3 ENRON datasets, reserving the last two sets for training the hamifiers/spamifiers. For the two splits, 10% will go into testing and the rest will be used for training. \n",
    "\n",
    "First, we need to be able to download the datasets from http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/preprocessed'\n",
    "\n",
    "\n",
    "def extract_to_numpy(set_path):\n",
    "    tar_file = requests.get(url + set_path, allow_redirects=True)\n",
    "    \n",
    "    tar_file = tarfile.open(fileobj=BytesIO(tar_file.content))\n",
    "    \n",
    "    message_bodies = []\n",
    "    message_labels = []\n",
    "    \n",
    "    def read_from_subdirectory(subdirectory):\n",
    "        is_spam = subdirectory == 'spam/'\n",
    "        \n",
    "        spam_files = [\n",
    "            tarinfo for tarinfo in tar_file.getmembers()\n",
    "            if re.match('\\w+/' + subdirectory + '\\w+', tarinfo.name)\n",
    "        ]\n",
    "\n",
    "        for spam_file in spam_files:\n",
    "            text_file = tar_file.extractfile(spam_file)\n",
    "\n",
    "            message_body = text_file.read()\n",
    "            \n",
    "            # Body may contain non ascii characters, which we want to remove.\n",
    "            message_body = ''.join(chr(char) for char in message_body if char<128)\n",
    "\n",
    "            message_bodies.append(message_body)\n",
    "            message_labels.append(is_spam)\n",
    "    \n",
    "    read_from_subdirectory('spam/')\n",
    "    read_from_subdirectory('ham/')\n",
    "    \n",
    "    message_bodies = np.array(message_bodies, dtype=str)\n",
    "    message_labels = np.array(message_labels, dtype=bool)\n",
    "    \n",
    "    return message_bodies, message_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the first dataset for the spam filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframe(dataframe, name, key):\n",
    "    if not path.isdir(Path('data/')):\n",
    "        # We are the first to store data, create directory.\n",
    "        os.mkdir('data')\n",
    "    \n",
    "    if not path.isdir(Path('data/enron/')):\n",
    "        # We are the first to store ENRON data, create directory.\n",
    "        os.mkdir('data/enron')\n",
    "    \n",
    "    # If file already exists, we are overwriting it here.\n",
    "    dataframe.to_hdf('data/enron/' + name, key=key, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_bodies, message_labels = extract_to_numpy('/enron1.tar.gz')\n",
    "\n",
    "spam_filter_data = pd.DataFrame(data={'BODY': message_bodies, 'SPAM': message_labels})\n",
    "\n",
    "spam_filter_train = spam_filter_data.sample(frac=0.9, random_state=100) # random_state is a seed value, don't change unless you want to refresh the splits\n",
    "spam_filter_test = spam_filter_data.drop(spam_filter_train.index)\n",
    "\n",
    "spam_filter_train = spam_filter_train.reset_index(drop=True)\n",
    "spam_filter_test = spam_filter_test.reset_index(drop=True)\n",
    "spam_filter_test.index = list(map(lambda i: i + len(spam_filter_train.index), spam_filter_test.index))\n",
    "\n",
    "save_dataframe(spam_filter_train, 'spam_filter_train.h5', 'spam_filter_train')\n",
    "save_dataframe(spam_filter_test, 'spam_filter_test.h5', 'spam_filter_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics for splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of spam in training set: 29.1%\n",
      "Percentage of spam in testing set: 27.9%\n"
     ]
    }
   ],
   "source": [
    "train_spam_proportion = spam_filter_train['SPAM'].value_counts(normalize=True)[True] * 100\n",
    "test_spam_proportion = spam_filter_test['SPAM'].value_counts(normalize=True)[True] * 100\n",
    "\n",
    "print('Percentage of spam in training set: ' + f'{train_spam_proportion:.1f}' + '%')\n",
    "print('Percentage of spam in testing set: ' + f'{test_spam_proportion:.1f}' + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the other two datasets for the hamifiers/spamifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_bodies, message_labels = extract_to_numpy('/enron3.tar.gz')\n",
    "\n",
    "hamifier_data = pd.DataFrame(data={'BODY': message_bodies, 'SPAM': message_labels})\n",
    "del message_bodies\n",
    "del message_labels\n",
    "\n",
    "hamifier_train = hamifier_data.sample(frac=0.9, random_state=100)\n",
    "hamifier_test = hamifier_data.drop(hamifier_train.index)\n",
    "\n",
    "hamifier_train = hamifier_train.reset_index(drop=True)\n",
    "hamifier_test = hamifier_test.reset_index(drop=True)\n",
    "hamifier_test.index = list(map(lambda i: i + len(hamifier_train.index), hamifier_test.index))\n",
    "\n",
    "save_dataframe(hamifier_train, 'hamifier_train.h5', 'hamifier_train')\n",
    "save_dataframe(hamifier_test, 'hamifier_test.h5', 'hamifier_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of spam in training set: 27.4%\n",
      "Percentage of spam in testing set: 26.0%\n"
     ]
    }
   ],
   "source": [
    "train_spam_proportion = hamifier_train['SPAM'].value_counts(normalize=True)[True] * 100\n",
    "test_spam_proportion = hamifier_test['SPAM'].value_counts(normalize=True)[True] * 100\n",
    "\n",
    "print('Percentage of spam in training set: ' + f'{train_spam_proportion:.1f}' + '%')\n",
    "print('Percentage of spam in testing set: ' + f'{test_spam_proportion:.1f}' + '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
